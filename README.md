# LLM_Chatbot_Gemini_wiki_pdf_Fil
Desarrollar un chatbot filosÃ³fico capaz de responder preguntas sobre autores, corrientes y conceptos filosÃ³ficos usando:  
- Contenido de Wikipedia (vÃ­a API).
- PDFs de manuales filosÃ³ficos (cargados y vectorizados).
- Gemini (vÃ­a LangChain) como modelo principal de respuesta y razonamiento.


# ğŸ§  Chatbot FilosÃ³fico con LangChain, Gemini, Wikipedia y PDFs

Este proyecto consiste en el desarrollo de un **chatbot inteligente orientado a responder preguntas filosÃ³ficas** utilizando modelos de lenguaje avanzados e informaciÃ³n proveniente de mÃºltiples fuentes. EstÃ¡ construido con LangChain como orquestador, Gemini como modelo LLM, y conocimiento extraÃ­do de Wikipedia y manuales en PDF.

---

## ğŸ¯ Objetivo

Crear un asistente conversacional capaz de responder consultas filosÃ³ficas de manera precisa, citando fuentes confiables y explicando conceptos complejos con claridad.

---

## ğŸ—ï¸ Arquitectura del Pipeline

```mermaid
flowchart TD
    A[Usuario] --> B[LangChain Orchestrator]
    B --> C[Retriever Wikipedia]
    B --> D[Retriever PDFs Vectorizados]
    C --> E[Contexto]
    D --> E
    E --> F[LLM: Gemini]
    F --> G[Respuesta Final]
```

---

## ğŸ”§ Componentes del pipeline

### 1. **Entrada del usuario**
- Pregunta libre relacionada con filosofÃ­a.

### 2. **LangChain Orchestrator**
- Determina si la respuesta se puede obtener desde Wikipedia, desde los PDFs, o ambas fuentes.
- Encadena el flujo de datos y llama a los retrievers y al modelo LLM.

### 3. **Wikipedia Retriever**
- Utiliza la API de Wikipedia a travÃ©s del wrapper de LangChain.
- Extrae resÃºmenes concisos sobre temas filosÃ³ficos en espaÃ±ol.

### 4. **PDF Retriever (con embeddings)**
- Procesa manuales filosÃ³ficos en PDF.
- Se vectorizan los documentos y se almacenan en FAISS o ChromaDB.
- Permite bÃºsquedas semÃ¡nticas.

### 5. **Modelo LLM (Gemini)**
- Toma el contexto recuperado y genera una respuesta clara, precisa y argumentada.
- Se puede usar vÃ­a `langchain-google-genai` o `ChatVertexAI`.

### 6. **Respuesta final**
- Se entrega al usuario, opcionalmente incluyendo referencias o fuentes consultadas.

---

## ğŸ“ Estructura del proyecto

```
chatbot_filosofico/
â”œâ”€â”€ data/                  # PDFs filosÃ³ficos
â”œâ”€â”€ vector_store/          # Base de datos vectorial (FAISS o Chroma)
â”œâ”€â”€ chains/                # LÃ³gica de recuperaciÃ³n y QA
â”‚   â””â”€â”€ rag_chain.py
â”œâ”€â”€ retrievers/
â”‚   â”œâ”€â”€ wikipedia.py
â”‚   â””â”€â”€ pdf_vector.py
â”œâ”€â”€ llm/
â”‚   â””â”€â”€ gemini.py
â”œâ”€â”€ main.py                # Orquestador principal
â”œâ”€â”€ config.yaml            # Configuraciones del sistema
â””â”€â”€ requirements.txt
```

---

## ğŸ› ï¸ TecnologÃ­as utilizadas

- **Python**
- **LangChain**
- **Gemini (Google Generative AI)**
- **WikipediaAPI**
- **FAISS / ChromaDB**
- **OpenAI o HuggingFace Embeddings**
- **PyMuPDF / pdfminer**
- **Streamlit** (para el frontend en la siguiente etapa)

---

## ğŸš€ PrÃ³ximos pasos

- [ ] Implementar frontend con Streamlit.
- [ ] Agregar historial conversacional.
- [ ] Agregar opciÃ³n de mostrar citas/referencias.
- [ ] Desplegar en un entorno web.

---

## ğŸ‘¨â€ğŸ’» Autor

**JerÃ³nimo MartÃ­nez** â€“ Data Scientist 
